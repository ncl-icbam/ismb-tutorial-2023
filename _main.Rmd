--- 
title: "Spatial transcriptomics data analysis: theory and practice"
author: "Eleftherios Zormpas, Dr Simon J. Cockell"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: "This book will guide you through the practical steps of the in-person tutorial IP2 for the ISMB/ECCB 2023 conference in Lyon named: Spatial transcriptomics data analysis: theory and practice."
link-citations: yes
github-repo: https://github.com/ncl-icbam/ismb-tutorial-2023.git
---

# Welcome {-}
This book will guide you through the practical steps of the in-person tutorial IP2 for the ISMB/ECCB 2023 conference in Lyon named: *"Spatial transcriptomics data analysis: theory and practice"*.

## Abstract {-}
Recent technological advances have led to the application of RNA Sequencing *in situ*. This allows for whole-transcriptome characterisation, at approaching single-cell resolution, while retaining the spatial information inherent in the intact tissue. Since tissues are congregations of intercommunicating cells, identifying local and global patterns of spatial association is imperative to elucidate the processes which underlie tissue function. Performing spatial data analysis requires particular considerations of the distinct properties of data with a spatial dimension, which gives rise to an association with a different set of *statistical* and *inferential* considerations. 

In this comprehensive tutorial, we will introduce users to spatial transcriptomics (STx) technologies and current pipelines of STx data analysis inside the **Bioconductor** framework. Furthermore, we will introduce attendees to the underlying features of spatial data analysis and how they can effectively utilise space to extract in-depth information from STx datasets.


## Learning objectives {-}

Participants in this tutorial will gain understanding of the core technologies for undertaking a spatial transcriptomics experiment, and the common tools used for the analysis of this data. In particular, participants will appreciate the strengths of geospatial data analysis methods in relation to this type of data. Specific learning objectives will include:

1.	Describe and discuss core technologies for spatial transcriptomics
2.	Make use of key computational technologies to process and analyse STx data
3.	Apply an analysis strategy to obtain derived results and data visualisations
4.	Appreciate the principles underlying spatial data analysis
5.	Understand some of the methods available for spatial data analysis
6.	Apply said methods to an example STx data set


```{r eval=FALSE, include=FALSE}
bookdown::serve_book()
```


```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

# Practical session 1
In this practical session you will familiarise yourself with the data and their different aspects.

## Log into the Posit Cloud server

## Import 10X Visium data
In this tutorial we will be using some already prepared data from the [STexampleData](https://bioconductor.org/packages/STexampleData) package. These data are stored in the `SpatialExperiment` format (the format is described below).

The dataset a single sample from the dorsolateral prefrontal cortex (DLPFC) 10x Genomics Visium dataset, that was published by @Maynard2021Mar.

Here, we show how to load the data from the `STexampleData` package.

```{r 01_load-data, message=FALSE}
library(SpatialExperiment)
library(STexampleData)

# Load the object
spe <- Visium_humanDLPFC()
```

## Explore data types
The below discussion will help you familiarise with the different data types that we will be using.

### SpatialExperiment class

For the first part of this tutorial (practical sessions 1 and 2), we will be using the  [SpatialExperiment](https://bioconductor.org/packages/SpatialExperiment) S4 class from Bioconductor as the main data structure for storing and manipulating datasets.

`SpatialExperiment` is a specialized object class that supports the storing of spatially-resolved transcriptomics datasets within the Bioconductor framework. It builds on the [SingleCellExperiment](https://bioconductor.org/packages/SingleCellExperiment) class [@Amezquita2020Feb] for single-cell RNA sequencing data. More specifically, has extra customisations to store spatial information (i.e., spatial coordinates and images).

An overview of the `SpatialExperiment` object structure is is presented in Figure 2.1. In brief, the `SpatialExperiment` object consists of the below five parts:  

 1. `assays`: gene expression counts  
 2. `rowData`: information about features, usually genes  
 3. `colData`: information on spots (non-spatial and spatial metadata)  
 4. `spatialCoords`: spatial coordinates  
 5. `imgData`: image data 

**NOTE:** For spot-based STx data (i.e., 10x Genomics Visium), a single `assay` named `counts` is used.

```{r SpExp-overview, echo=FALSE, out.width = "100%", fig.align="center", fig.cap="Overview of the `SpatialExperiment` object class structure."}
knitr::include_graphics("images/SpatialExperiment.png")
```

For more details, see the related publication from Righelli et al., 2021 describing the `SpatialExperiment` [@Righelli2022Jun].

### Inspect the object
```{r 01_data-inspect, message=FALSE}
## Check the object's structure
spe

## Check number of features/genes (rows) and spots (columns)
dim(spe)

## Check names of 'assay' tables
assayNames(spe)
```

### Counts table and gene metadata
```{r 01_counts-inspect, message=FALSE}
## Have a look at the counts table
assay(spe)[1:6,1:4]
```

As you can see here the counts table is of class `dgTMatrix` which is a sparse matrix. This is because much like scRNA-seq data, STx data are sparse and include many zeros. As a result, to make the counts table as light as possible we resort to using sparse matrices. The next code chunk will demonstrate a part of the matrix that includes genes with some level of expression.

```{r 01_counts-chunks, message=FALSE}
assay(spe)[20:40, 2000:2010]

assay(spe)[33488:33508, 2000:2010]
```

As you can see, the levels of expression of different genes in the same spots differ significantly with many low values being present. We have to remember here that we look at un-normalised expression data which are affected by factors like library size. Nonetheless, this is a fact in STx data as it is for scRNA-seq data; many genes will show low expression in individual spots.

```{r 01_gene-metaData, message=FALSE}
## Have a look at the genes metadata
head(rowData(spe))

```

### Coordinates table and spot metadata
```{r 01_coordinates-inspect, message=FALSE}
## Check the spatial coordinates
head(spatialCoords(spe))

## spot-level metadata
head(colData(spe))

```

### Image metadata
```{r 01_image-inspect, message=FALSE}
## Have a look at the image metadata
imgData(spe)
```



<!--chapter:end:01-intro-to-stx.Rmd-->

# Practical session 2
In this session we will demonstrate the implementation of the methods discussed earlier and will particularly focus on the most common analysis routines in STx: QC, data visualisation and clustering analysis always inside the interoperable Bioconductor environment.

```{r 02_loadPackages, , message=FALSE}
## Load packages {-}
library(ggspavis)
library(ggplot2)
library(scater)
library(scran)
library(igraph)
library(pheatmap)
```

- [`ggspavis`](https://bioconductor.org/packages/release/bioc/html/ggspavis.html) is a Bioconductor package that includes visualization functions for spatially resolved transcriptomics datasets stored in `SpatialExperiment` format from spot-based (e.g., 10x Genomics Visium) platforms (@ggspavis2023Apr).

- [`scater`](https://bioconductor.org/packages/release/bioc/html/scater.html) is also a Bioconductor package that is a selection of tools for doing various analyses of scRNA-seq gene expression data, with a focus on quality control and visualization which has extended applications to STx data too. It is based on the `SingleCellExperiment` and `SpatialExperiment` classes and thus is interoperable with many other Bioconductor packages such as [`scran`](https://bioconductor.org/packages/3.16/scran), [`scuttle`](https://bioconductor.org/packages/3.16/scuttle) and [`iSEE`](https://bioconductor.org/packages/3.16/iSEE).


## Spot-level Quality Control
Spot-level quality control (sQC) procedures are employed to eliminate low-quality spots before conducting further analyses. Low-quality spots may result from issues during library preparation or other experimental procedures, such as a high percentage of dead cells due to cell damage during library preparation, or low mRNA capture efficiency caused by ineffective reverse transcription or PCR amplification. Keeping these spots usually leads to creating problems during downstream analyses.

We can identify low-quality spots using several characteristics that are also used in QC for scRNA-sq data, including:

1. **library size** (total of UMI counts per spot is going to be different due to sequencing *-like different samples in a bulk RNA-seq-* or due to number of cells in the spot)
2. **number of expressed genes** (i.e. number of genes with non-zero UMI counts per spot)
3. **proportion of reads mapping to mitochondrial genes** (a high proportion indicates putative cell damage)

Low library size or low number of expressed features can indicate poor mRNA capture rates, e.g. due to cell damage and missing mRNAs, or low reaction efficiency. A high proportion of mitochondrial reads indicates cell damage, e.g. partial cell lysis leading to leakage and missing cytoplasmic mRNAs, with the resulting reads therefore concentrated on the remaining mitochondrial mRNAs that are relatively protected inside the mitochondrial membrane. Unusually high numbers of cells per spot can indicate problems during cell segmentation.

The idea of using scRNA-seq QC metrics in STx data comes from the fact that if we remove space and count each spot as a single cell, the two datasets share common features. However, the expected distributions for high-quality spots are different (compared to high-quality cells in scRNA-seq), since spots may contain zero, one, or multiple cells.

A few publications for further reading that can help you understand the quality controls: @McCarthy2017Apr and @Amezquita2020Feb.

### Plot tissue map
The DLPFC dataset we will be using comes with manual annotations by the authors @Maynard2021Mar. We can plot the tissue map with and without the annotations to get a complete view.

```{r 02_plot-maps-gTruth, fig.show = 'hold', out.width="50%", fig.height=5, fig.width=4}
## Plot spatial coordinates without annotations
plotSpots(spe)

## Plot spatial coordinates with annotations
plotSpots(spe,
          annotate = "ground_truth")
```

### Calculating QC metrics
We will now calculate the three main QC metrics described above using methods from the `scater` [@McCarthy2017Apr] package and our own functions **INSERT LINK OR CITATION HERE**.

So far, the dataset contains both on- and off-tissue spots. For the analysis though we are only interested in the on-tissue spots. Therefore, before we run any calculations we want to remove the off-tissue spots.

***NOTE***: the on- or off-tissue information for each spot can be found in the `colData` of the `spe` object and in the `in_tissue` column where *0 = off-tissue* and *1 = on-tissue*.

```{r 02_keep_on-tissue}
## Dataset dimensions before the filtering
dim(spe)

## Subset to keep only on-tissue spots
spe <- spe[, colData(spe)$in_tissue == 1]
dim(spe)
```

The next thing we need to do before we make decisions on how to quality *"trim"* the dataset is to calculate the percentage per spot of mitochodrial gene expression and store this information inside the `colData`.

```{r 02_find-mitoGenes}
## Fetch mitochondrial gene names
is_mito <- grepl("(^MT-)|(^mt-)", rowData(spe)$gene_name)
rowData(spe)$gene_name[is_mito]
```

```{r 02_perCellQCs}
## Calculate per-spot QC metrics and store in colData
spe <- addPerCellQC(spe, subsets = list(mito = is_mito))
head(colData(spe))
```

After calculating the necessary metrics, we need to apply some cut-off thresholds for each metric to perform QC over each spot. What is important to remember here is that each dataset might need slightly different cut-off values to be applied. As a result we cannot rely on identifying a single value to use every time and we need to rely on plotting these metrics and making a decision on a dataset-by-dataset basis.

### Library size threshold plot
We can plot a histogram of the library sizes across spots. The library size is the number of UMI counts in each spot. We can find this information in the `sum` column in the `colData`.

```{r 02_plot-libSize-histo, fig.height=3.5, warning=FALSE, message=FALSE}
## Density and histogram of library sizes
ggplot(data = as.data.frame(colData(spe)),
       aes(x = sum)) +
  geom_histogram(aes(y = after_stat(density)), 
                 colour = "black", 
                 fill = "grey") +
  geom_density(alpha = 0.5,
               adjust = 0.5,
               fill = "#A0CBE8",
               colour = "#4E79A7") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) + 
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) + 
  xlab("Library size") + 
  ylab("Density") + 
  theme_classic()
```

As we can see there are no obvious issues with the library sizes. An example of an issue could be a high frequency of small libraries which would indicate poor experimental output. Generally we do not want to keep spots with too small libraries.

If the dataset we are analysing contains the number of cells that are present in each spot (this one does), then it makes sense to also plot the library sizes against the number of cells per spot. In that way we are making sure that we don't remove any spots that may have biological meaning. In many cases though the datasets do not have such information unless we can generate it using a nuclei segmentation tool to extract this information from the H&E images.

The horizontal red line (argument `threshold` in the `plotQC` function) shows a first guess at a possible filtering threshold for library size based on the above histogram. The below plot can also be plotted using ggplot.

```{r 02_plot-libSizeVScelNo, fig.width=6, fig.height=5, warning=FALSE, message=FALSE}
## Scatter plot, library size against number of cells per spot
plotQC(spe, type = "scatter", 
       metric_x = "cell_count", metric_y = "sum", 
       threshold_y = 700)
```

We need to keep in mind here that the threshold is, up to an extend, arbitrary. It is crucial, then, to have a look at the number of spots that are left out of the dataset because of this cut-off value and also have a look at their putative spatial patterns. If, by any chance, we filtered out spots with biological relevance, then we will observe some patterns on the tissue map that are correlating with some of the known biological structures of the tissue. As a result, we probably have set our threshold too high.

```{r 02_libSize-thresh, fig.height=4}
## Select library size threshold
qc_lib_size <- colData(spe)$sum < 700
## Check how many spots are filtered out
table(qc_lib_size)
## Add threshold in colData
colData(spe)$qc_lib_size <- qc_lib_size

## Check putative spatial patterns of removed spots
plotQC(spe, type = "spots", 
       discard = "qc_lib_size")
```

As an aside, try to illustrate what happens if we set the threshold too high (i.e., 2000 UMI counts).  
**NOTE:** For reference, remember the ground truth layers in this dataset that we [plotted][Plot tissue map] at the beginning.

```{r 02_exercise01, fig.height=4, eval=FALSE}
## Select library size threshold
code...
## Check how many spots are filtered out
code...
## Add threshold in colData
code...

## Check putative spatial patterns of removed spots
plotQC(...)
```


### Number of expressed genes
As we did with the library sizes, we can plot a histogram of the number of expressed genes across spots. A gene is expressed in a spot if it has at least one count in it. We can find this information in the `detected` column in the `colData`.

We will follow the same logic for the plots as we did for the library size earlier.

```{r 02_plot-genesInSpot-histo, fig.height=4, warning=FALSE, message=FALSE}
## Density and histogram of expressed genes
ggplot(data = as.data.frame(colData(spe)),
       aes(x = detected)) +
  geom_histogram(aes(y = after_stat(density)), 
                 colour = "black", 
                 fill = "grey") +
  geom_density(alpha = 0.5,
               adjust = 0.5,
               fill = "#A0CBE8",
               colour = "#4E79A7") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) + 
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) + 
  xlab("Genes expressed in each spot") + 
  ylab("Density") + 
  theme_classic()
```

```{r 02_genesInSpot-scatter, fig.width=6, fig.height=5, warning=FALSE, message=FALSE}
# plot number of expressed genes vs. number of cells per spot
plotQC(spe, type = "scatter", 
       metric_x = "cell_count", metric_y = "detected", 
       threshold_y = 500)
```

```{r 02_genesInSpot-thresh, fig.height=4}
## Select expressed genes threshold
qc_detected <- colData(spe)$detected < 500
## Check how many spots are filtered out
table(qc_detected)
## Add threshold in colData
colData(spe)$qc_detected <- qc_detected

## Check for putative spatial pattern of removed spots
plotQC(spe, type = "spots", 
       discard = "qc_detected")
```

Again, try to illustrate what happens if we set the threshold too high (i.e., 1000 expressed genes).  
**NOTE:** For reference, remember the ground truth layers in this dataset that we [plotted][Plot tissue map] at the beginning.

```{r 02_exercise02, fig.height=4, eval=FALSE}
## Select library size threshold
code...
## Check how many spots are filtered out
code...
## Add threshold in colData
code...

## Check putative spatial patterns of removed spots
plotQC(...)
```

### Percentage of mitochondrial expression
As we briefly touched at the beginning, a high proportion of mitochondrial reads indicates low cell quality, probably due to cell damage.

To investigate the percentage of mitochondrial expression across spots we need to take a look in the column `subsets_mito_percent` in the `colData`.

```{r 02_plot-mitoPercent-histo, fig.height=4, warning=FALSE, message=FALSE}
## Density and histogram of percentage of mitochondrial expression
ggplot(data = as.data.frame(colData(spe)),
       aes(x = subsets_mito_percent)) +
  geom_histogram(aes(y = after_stat(density)), 
                 colour = "black", 
                 fill = "grey") +
  geom_density(alpha = 0.5,
               adjust = 0.5,
               fill = "#A0CBE8",
               colour = "#4E79A7") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) + 
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) + 
  xlab("Percentage of mitochondrial expression") + 
  ylab("Density") + 
  theme_classic()
```

```{r 02_mitoPercent-scatter, fig.width=6, fig.height=5, warning=FALSE, message=FALSE}
# plot mitochondrial read proportion vs. number of cells per spot
plotQC(spe, type = "scatter", 
       metric_x = "cell_count", metric_y = "subsets_mito_percent", 
       threshold_y = 28)
```

```{r 02_mitoPercent-thresh, fig.height=4}
## Select expressed genes threshold
qc_mito <- colData(spe)$subsets_mito_percent > 28
## Check how many spots are filtered out
table(qc_mito)
## Add threshold in colData
colData(spe)$qc_mito <- qc_mito

## Check for putative spatial pattern of removed spots
plotQC(spe, type = "spots", 
       discard = "qc_mito")
```

Again, try to illustrate what happens if we set the threshold too low (i.e., 20 0r 25%).  
**NOTE:** For reference, remember the ground truth layers in this dataset that we [plotted][Plot tissue map] at the beginning.

```{r 02_exercise03, fig.height=4, eval=FALSE}
## Select library size threshold
code...
## Check how many spots are filtered out
code...
## Add threshold in colData
code...

## Check putative spatial patterns of removed spots
plotQC(...)
```

### Number of cells per spot
Number of cells per spot is an attribute that not all datasets include. Nonetheless, if it exists, we can use this information to further control the quality of the dataset prior to any downstream analysis. Ofcourse, the number of cells per spot depends on the tissue type and organism and according to [10X Genomics](https://kb.10xgenomics.com/hc/en-us/articles/360035487952-How-many-cells-are-captured-in-a-single-spot-), each spot can contain between 0 and 10 cells.

To investigate the number of cells in each spot looking for any outlier values that could indicate problems we need to take a look in the column `cell_count` in the `colData`.

```{r 02_plot-cellsPerSpot-histo, fig.height=4, warning=FALSE, message=FALSE}
## Density and histogram of the number of cells in each spot
ggplot(data = as.data.frame(colData(spe)),
       aes(x = cell_count)) +
  geom_histogram(aes(y = after_stat(density)), 
                 colour = "black", 
                 fill = "grey") +
  geom_density(alpha = 0.5,
               adjust = 1.5,
               fill = "#A0CBE8",
               colour = "#4E79A7") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) + 
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) + 
  xlab("Number of cells per spot") + 
  ylab("Density") + 
  theme_classic()

## Have a look at the values
table(colData(spe)$cell_count)
```

```{r 02_cellsPerSpot-scatter, fig.width=6, fig.height=5, warning=FALSE, message=FALSE}
# plot number of expressed genes vs. number of cells per spot
plotQC(spe, type = "scatter", 
       metric_x = "cell_count", metric_y = "detected", 
       threshold_x = 10)
```

As we can see from both the histogram and the scatter plot there is a tail of very high values, which could indicate problems for these spots. More specifically, we can see from the scatter plot that most of the spots with very high cell counts also have low numbers of expressed genes. This indicates problems with the experiment on these spots, and they should be removed.

```{r 02_cellsPerSpot-thresh, fig.height=4}
## Select expressed genes threshold
qc_cell_count <- colData(spe)$cell_count > 10
## Check how many spots are filtered out
table(qc_cell_count)
## Add threshold in colData
colData(spe)$qc_cell_count <- qc_cell_count

## Check for putative spatial pattern of removed spots
plotQC(spe, type = "spots", 
       discard = "qc_cell_count")
```

While there is a spatial pattern to the discarded spots, it does not appear to be correlated with the known biological features (cortical layers). The discarded spots are all on the edges of the tissue. It seems plausible that something has gone wrong with the cell segmentation on the edges of the images, so it makes sense to remove these spots.

The discarded spots are located at the tissue edges, indicating a potential issue with cell segmentation in those regions. Therefore, it is reasonable to remove these spots from the analysis.

### Remove low-quality spots

Since we have calculated different spot-level QC metrics and selected thresholds for each one, we can combine them to identify a set of low-quality spots, and remove them from our `spe` object.

We also check again that the combined set of discarded spots does not correspond to any obvious biologically relevant group of spots.

```{r 02_checkQC-thresh, fig.height=4}
## Check the number of discarded spots for each metric
apply(cbind(qc_lib_size, qc_detected, qc_mito, qc_cell_count), 2, sum)
## Combine together the set of discarded spots
discard <- qc_lib_size | qc_detected | qc_mito | qc_cell_count
## Store the set in the object
colData(spe)$discard <- discard

## Check the spatial pattern of combined set of discarded spots
plotQC(spe, type = "spots", 
       discard = "discard")
```
Since this dataset has also manual annotation (remember [here][Plot tissue map]) we see that there are locations that are not annotated (marked with `NA`). We could further remove those locations to reduce noise and further increase the quality of the dataset.
```{r 02_notAnnotSpots, fig.height=4}
## Select locations without annotation
qc_NA_spots <- is.na(colData(spe)$ground_truth)
## Combine together the set of discarded spots
discard <- qc_lib_size | qc_detected | qc_mito | qc_cell_count | qc_NA_spots
## Store the set in the object
colData(spe)$discard <- discard

## Check the spatial pattern of combined set of discarded spots
plotQC(spe, type = "spots", 
       discard = "discard")
```
```{r 02_applyFilter}
## remove combined set of low-quality spots
spe <- spe[, !colData(spe)$discard]
```

## Normalisation of counts
### Background
The most common method (if not the only one so far) of normalising gene expression in STx data is the method used in scRNA-seq data. Namely, this is a log-transformation (logcounts). For this to be applied, we treat each spot as being a single cell.

It is clear from what we discussed [earlier][Number of cells per spot] that each spot can contain more than one cells. This is a limitation to the technology itself and to our methods of analysing STx data so far. Nonetheless, since STx expression data look like scRNA-seq we apply scRNA-seq methods for normalising (not all methods can be applied though). 

Here we will be using methods from the `scater` [@McCarthy2017Apr] and `scran` [@Lun2016Oct] packages that calculate logcounts using library size factors. The library size factors approach is arguably the simplest approach for STx data. Other approaches used in scRNA-seq are more difficult to justify their use in STx because of two main reasons:

  1. Spots can contain multiple cells of different cell-types.
  2. Datasets can include multiple tissue samples which will lead to different clusterings.

### Log-tranformation of counts
```{r 02_libraryFactors, message=FALSE}
## Calculate library size factors
spe <- computeLibraryFactors(spe)
## Have a look at the size factors
summary(sizeFactors(spe))
```

```{r 02_plot-labfact-histo, fig.height=4, warning=FALSE, message=FALSE}
## Density and histogram of library sizes
ggplot(data = data.frame(sFact = sizeFactors(spe)), 
       aes(x = sFact)) +
  geom_histogram(aes(y = after_stat(density)), 
                 colour = "black", 
                 fill = "grey") +
  geom_density(alpha = 0.5,
               adjust = 0.5,
               fill = "#A0CBE8",
               colour = "#4E79A7") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) + 
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) + 
  xlab("Library size") + 
  ylab("Density") + 
  theme_classic()
```


The log-transformation that takes place is a log2-transformation and in order to avoid *- Infinity* values we add a pseudo value of 1. Both the log2- transformation and the pseudocount of 1 are defaults in this method.
```{r 02_logNormCounts}
## Calculate logcounts and store in the spe object
spe <- logNormCounts(spe)

## Check that a new assay has been added
assayNames(spe)
```

## Selecting genes
### Background
Gene selection -or alternatively "feature selection"- is applied to identify genes that are going to be informative and can produce valuable information form the downstream analyses. The most common way of applying gene selection is to select genes that are highly variable (HVGs). The assumption is that since we quality-controled and normalised our dataset, the genes with high variability are the ones that contain high levels of biological variability too. Since here we have a spatial dataset we can also try to identify spatially variable genes too (SVGs).

What is important to note here is that HVGs are identified using features like gene expression. Spatial information does not play a role in finding HVGs. STx data pose a dilemma; does the meaningful spatial information reflects only spatial distribution of major cell types or reflects additional important spatial features? If the first is true, then, relying on HVGs can be enough. If the second also holds true, then, it is important to identify SVGs as well.

### Highly Variable Genes (HVGs)

Here we will be using methods from the `scran` package [@Lun2016Oct] to identify a set of HVGs. Again, here we need to remember that `scran` methods were developed for scRNA-seq and we are performing the analysis under the assumption that the spots of an STx experiment can be treated as single cells.

In this dataset, the mitochondrial genes are too highly expressed and are not of major biological interest. As a result, if we are to identify true HVGs, we first need to remove the mitochondrial genes.

```{r 02_features_remvMito, message=FALSE}
## Remove mitochondrial genes
spe <- spe[!is_mito, ]
```


Then, we apply methods from `scran` that give a list of HVGs, which can be used for further downstream analyses. 

First we model the variance of the log-expression profiles for each gene, decomposing it into technical and biological components based on a fitted mean-variance trend.

```{r 02_features_FitModel, message=FALSE, fig.height=5}
## Fit mean-variance relationship
dec <- modelGeneVar(spe)
## Visualize mean-variance relationship
fit <- metadata(dec)
fit_df <- data.frame(mean = fit$mean,
                     var = fit$var,
                     trend = fit$trend(fit$mean))

ggplot(data = fit_df, 
       aes(x = mean, y = var)) + 
  geom_point() + 
  geom_line(aes(y = trend), colour = "dodgerblue", linewidth = 1.5) + 
  labs(x = "mean of log-expression",
       y = "variance of log-expression") + 
  theme_classic()
```
The `trend` function that we used above is returned from the `modelGeneVar` function and returns the fitted value of the trend at any value of the mean.

We select the top 10% of genes based on their variability The parameter `prop` defines how many HVGs we want. For example `prop = 0.1` returns the top 10% of genes.
```{r 02_features_selectHVGs}
## Select top HVGs
top_hvgs <- getTopHVGs(dec, prop = 0.1)

## How many are the HVGs?
length(top_hvgs)
```


### Spatially variable genes (SVGs)

SVGs are genes with a highly spatially correlated pattern of expression, which varies along with the spatial distribution of a tissue structure of interest. This phenomenon is also called *spatial autocorrelation* and is a phenomenon that underlies all types of spatial data as we will discuss later.

The field of geography has developed some statistical measures to calculate spatial autocorrelation. Examples of these are Moran's *I* [@Moran1950Jun] and Geary's *C* [@Geary1954Nov] that can be used to rank genes by the observed spatial autocorrelation to identify SVGs.

Several sophisticated new statistical methods to identify SVGs in SRT data have also recently been developed. These include [SpatialDE](https://github.com/Teichlab/SpatialDE) [@Svensson2018May], [SPARK](https://xzhoulab.github.io/SPARK/) [@Sun2020Feb], and [SPARK-X](https://xzhoulab.github.io/SPARK/) [@Zhu2021Dec].


### Integration of HVGs and SVGs

A recent benchmark paper [@Li2022Jan] showed that integrating HVGs and SVGs to generate a combined set of features can improve downstream clustering performance in SRT data. This confirms that SVGs contain additional biologically relevant information that is not captured by HVGs in these datasets. For example, a simple way to combine these features is to concatenate columns of principal components (PCs) calculated on the set of HVGs and the set of SVGs (excluding overlapping HVGs), and then using the combined set of features for further downstream analyses [@Li2022Jan].

## Dimensionality reduction
### Background  

Dimensionality reduction is an important step prior to any downstream clustering attempts. There are two main ways of reducing the dimensions of a dataset. The gold standard is Principal Components Analysis (PCA) and the other -a more recent one- Uniform manifold Approximation and Projection (UMAP) [@McInnes2018Feb]. The main difference between the two is that the distances between the data points in PCA space are interpretable and can be used to cluster the data points while the distances in a UMAP embedding are not interpretable and thus, cannot be used to cluster the data points. As a result, we will be using PCA to reduce the dimensions of our dataset to assist clustering and UMAP to further reduce the principal components (PCs) in a two-dimensional space and produce better visualisations for the PCA.

Reducing the dimensions of the dataset is the output of a PCA. But, what are the main reasons we do so? One reason is to reduce the noise introduced by biologically uninteresting genes that their expression might show some random variation -and that is why they are in the HVGs list-. The other reason is to improve the computational efficiency of downstream analyses like clustering. In an STx experiment, like the one we are analysing here, we have more than 3000 spots and almost 1500 HVGs. As as result, each spot has 1500 attributes based on which the clustering will take place. This increase in the number of variables that differentiate or cluster together spots is leading to the *curse of dimensionality* [@Keogh2017Apr] which makes the data points (spots) look equidistant in attribute space resulting in poor clustering output. 

### PCA: Principal component analysis

Here we will use an efficient implementation of PCA provided in the `scater` package [@McCarthy2017Apr] and retain the top 50 PCs for further downstream analyses. The random seed is required for reproducibility reasons because this implementation uses randomisation.

```{r 02_dimRedct_PCA}
## Set seed
set.seed(987)
## Compute PCA
spe <- runPCA(spe, subset_row = top_hvgs)
## Check correctness - names
reducedDimNames(spe)
## Check correctness - dimensions
dim(reducedDim(spe, "PCA"))
```
 

### UMAP: Uniform Manifold Approximation and Projection

Here we will also run UMAP -using `scater`'s implementation- on the 50 PCs generated above and retain the top 2 UMAP components to visualise results.

```{r 02_dimRed_UMAP}
## Set seed
set.seed(987)
## Compute UMAP on top 50 PCs
spe <- runUMAP(spe, dimred = "PCA")
## Check correctness - names
reducedDimNames(spe)
## Check correctness - dimensions
dim(reducedDim(spe, "UMAP"))
## Update column names for easier plotting
colnames(reducedDim(spe, "UMAP")) <- paste0("UMAP", 1:2)
```

### UMAP visualisations

We can generate plots either using plotting functions from the [ggspavis](https://bioconductor.org/packages/ggspavis) package or [`ggplot2`](https://ggplot2.tidyverse.org/) package. Later on clustering, we will add cluster labels to these reduced dimension plots for an off-tissue visualisation.

```{r 02_dimRed_UMAP-vis, fig.width=6, fig.height=5, fig.show='hold'}
## Plot top 2 PCA dimensions
# plotDimRed(spe, type = "PCA")

ggplot(data = as.data.frame(spe@int_colData@listData$reducedDims$PCA),
       aes(x = PC1, y = PC2, colour = spe@colData$ground_truth)) + 
  geom_point(size = 0.5) + 
  scale_colour_brewer(type = "qual") + 
  labs(title = "Reduced dimensions: PCA",
       x = "PC1",
       y = "PC2",
       colour = "Layers") +
  theme_classic()

## Plot top 2 UMAP dimensions
# plotDimRed(spe, type = "UMAP")

ggplot(data = as.data.frame(spe@int_colData@listData$reducedDims$UMAP),
       aes(x = UMAP1, y = UMAP2, colour = spe@colData$ground_truth)) + 
  geom_point(size = 0.5) + 
  scale_colour_brewer(type = "qual") + 
  labs(title = "Reduced dimensions: UMAP",
       x = "UMAP1",
       y = "UMAP2",
       colour = "Layers") +
  theme_classic()
```


## Clustering
### Background

The clustering of observations into statistically similar groups is a well-established application in both bulk and single-cell RNA-Seq analysis. Clustering is a helpful tool because it structures and orders the data, allowing useful insights to be gained from complex, multivariate datasets and use those insights to classify the observed data or to generate hypotheses.

Common clustering methods are applied to ST data based on correlation or statistical distance of gene expression measurements. As we briefly touched above, the dimensionality of ST data means that sample distances in gene expression space tend to be small and not reliable for identifying clusters, so feature selection (gene selection) or dimensionality reduction approaches (i.e., PCA, UMAP) tend to be taken before clustering.

Common approaches to clustering gene expression data include k-means, hierarchical and Louvain algorithms, and all have been applied to the clustering of ST data. Some of these methods are implemented in some of the most popular single-cell analysis packages, such as `Seurat` [@Hao2021Jun] and `scran` [@Lun2016Oct] and have been used for clustering in a number of ST studies.

### Clustering on HVGs

Here, we apply graph-based clustering to the top 50 PCs calculated on the set of selected HVGs, using the Walktrap method implemented in `scran` [@Lun2016Oct]. To do so, we assume that (i) each spot is equal to a cell and (ii) we can detect from the gene expression the biologically informative spatial distribution patterns of cell types.

```{r 02_clustering}
## Set seed
set.seed(987)
## Set number of Nearest-Neighbours (NNs)
k <- 10
## Build the k-NN graph
g <- buildSNNGraph(spe, k = k, use.dimred = "PCA")
## Run walktrap clustering
g_walk <- igraph::cluster_walktrap(g)
## Get the cluster labels
clus <- g_walk$membership
## Check how many
table(clus)
## Store cluster labels in column 'label' in colData
colLabels(spe) <- factor(clus)
```

### HVGs clustering visualisations

We can visualise the clusters in two ways:  
1. plotting in spatial coordinates on the tissue map
2. plotting in the UMAP/PCA embeddings.

We can use plotting functions either from the [ggspavis](https://bioconductor.org/packages/ggspavis)package.

For reference, we will also display the ground truth (manually annotated) labels available for this dataset.

```{r 02_clust_vis-map, fig.show = 'hold', out.width="50%", fig.height=5, fig.width=4}
## Plot in tissue map
plotSpots(spe, annotate = "label", 
          palette = "libd_layer_colors")

## Plot ground truth in tissue map
plotSpots(spe, annotate = "ground_truth", 
          palette = "libd_layer_colors")
```

```{r 02_clust_vis-DimRed, message=FALSE, fig.show = 'hold', out.width="50%", fig.height=4, fig.width=4}
## Plot clusters in PCA space
plotDimRed(spe, type = "PCA", 
           annotate = "label", palette = "libd_layer_colors")

## Plot clusters in UMAP space
plotDimRed(spe, type = "UMAP", 
           annotate = "label", palette = "libd_layer_colors")
```

From the visualizations, we can see that the clustering reproduces, up to an extend, the known biological structure of the tissue, but not perfectly. One reason for this could be the fact that each spot may comprise of many different cells whose gene expression profiles are diluted in the overall profile of the spot, thus leading to low-quality clustering.

### Spatially-aware clustering

In STx data, we can also perform clustering that takes spatial information into account, for example to identify spatially compact or spatially connected clusters.

A simple strategy is to perform graph-based clustering on a set of features (columns) that includes both molecular features (gene expression) and spatial features (x-y coordinates). In this case, a crucial tuning parameter is the relative amount of scaling between the two data modalities -- if the scaling is chosen poorly, either the molecular or spatial features will dominate the clustering. Depending on data availability, further modalities could also be included. In this section, we will include some examples on this clustering approach.

## Inter-cluster differentially expressed genes (DGEs)
### Background

Here, we will identify differentially expressed genes bewteen clusters.

We will use the `findMarkers` implementation from the `scran` [@Lun2016Oct]. This implementation uses a binomial test, which tests for genes that differ in the proportion expressed vs. not expressed between clusters. This is a more stringent test than the default t-tests, and tends to select genes that are easier to interpret and validate experimentally.

### DGEs identification

```{r 02_dges}
## Set gene names as row names ease of plotting
rownames(spe) <- rowData(spe)$gene_name
## Test for DGEs
markers <- findMarkers(spe, test = "binom", direction = "up")
## Check output
markers
```

The output from the `findMarkers` implementation is a list of length equal to the number of clusters. Each element of the list contains the Log-Fold-Change (LogFC) of each gene between one cluster and all others.

### DGEs visualisation

Here we will plot LogFCs for cluster 1 against all other clusters

```{r 02_dges_vis-clst1, message=FALSE, fig.show = 'hold', out.width="50%", fig.height=4, fig.width=4}
## Select cluster 1 genes
interesting <- markers[[1]]
## Get the top genes
best_set <- interesting[interesting$Top <= 5, ]
## Calculate the effect
logFCs <- getMarkerEffects(best_set)
## Plot a heat map
pheatmap(logFCs, breaks = seq(-5, 5, length.out = 101))
```

Below we will plot the log-transformed normalised expression of the top genes for one cluster alongside their expression in the other clusters.

```{r 02_dges_vis-2, message=FALSE, fig.width=7, fig.height=7}
## Select genes
top_genes <- head(rownames(interesting))
## Plot expression
plotExpression(spe, x = "label", features = top_genes)
```

## Other visualisations

## Putting it all together
```{r all_together}
# clear workspace from previous chapters
rm(list = ls(all = TRUE))

# LOAD DATA

library(SpatialExperiment)
library(STexampleData)
spe <- Visium_humanDLPFC()

# QUALITY CONTROL (QC)

library(scater)
# subset to keep only spots over tissue
spe <- spe[, colData(spe)$in_tissue == 1]
# identify mitochondrial genes
is_mito <- grepl("(^MT-)|(^mt-)", rowData(spe)$gene_name)
# calculate per-spot QC metrics
spe <- addPerCellQC(spe, subsets = list(mito = is_mito))
# select QC thresholds
qc_lib_size <- colData(spe)$sum < 600
qc_detected <- colData(spe)$detected < 400
qc_mito <- colData(spe)$subsets_mito_percent > 28
qc_cell_count <- colData(spe)$cell_count > 10
# combined set of discarded spots
discard <- qc_lib_size | qc_detected | qc_mito | qc_cell_count
colData(spe)$discard <- discard
# filter low-quality spots
spe <- spe[, !colData(spe)$discard]

# NORMALIZATION

library(scran)
# calculate logcounts using library size factors
spe <- logNormCounts(spe)

# FEATURE SELECTION

# remove mitochondrial genes
spe <- spe[!is_mito, ]
# fit mean-variance relationship
dec <- modelGeneVar(spe)
# select top HVGs
top_hvgs <- getTopHVGs(dec, prop = 0.1)

# DIMENSIONALITY REDUCTION

# compute PCA
set.seed(123)
spe <- runPCA(spe, subset_row = top_hvgs)
# compute UMAP on top 50 PCs
set.seed(123)
spe <- runUMAP(spe, dimred = "PCA")
# update column names
colnames(reducedDim(spe, "UMAP")) <- paste0("UMAP", 1:2)

# CLUSTERING

# graph-based clustering
set.seed(123)
k <- 10
g <- buildSNNGraph(spe, k = k, use.dimred = "PCA")
g_walk <- igraph::cluster_walktrap(g)
clus <- g_walk$membership
colLabels(spe) <- factor(clus)

# MARKER GENES
# test for marker genes
rownames(spe) <- rowData(spe)$gene_name
markers <- findMarkers(spe, test = "binom", direction = "up")
```

<!--chapter:end:02-stx-analysis.Rmd-->

# Practical session 3
This practical session will demonstrate the application of the most commonly used spatial analysis tools to STx data, and how we work with coordinate data alongside expression data. 

## Load packages
```{r 03_loadPackages, , message=FALSE}
library(spdep)
library(sf)
library(GWmodel)
library(ggplot2)
library(tidyverse)
```

- [`spdep`](https://cran.r-project.org/web/packages/spdep/index.html) is a collection of functions to create spatial weights matrix objects from polygon *'contiguities'*, from point patterns by distance and tessellations, for summarizing these objects, and for permitting their use in spatial data analysis like regional aggregation and tests for spatial *'autocorrelation'*.

- [`sf`](https://cran.r-project.org/web/packages/sf/index.html) (*Simple Features for R*) is a package that offers support for simple features, a standardized way to encode spatial vector data.

- [`GWmodel`](https://cran.r-project.org/web/packages/GWmodel/index.html) is a suit of models that fit situations when data are not described well by some global model, but where there are spatial regions where a suitably localised calibration provides a better description.

## Background
### Main geocomputatinal data structures
There are three main data structures that we need to have ready before we undertake a geocomputational approach to STx data analysis. Namely these are; (1) geometries (point and polygon), (2) neighbours lists and (3) distance matrices.

  1. Spatial geometries can be points, lines, polygons and pixels. Polygons consist of a multitude of points connected by lines and can have many forms like circle, hexagon, non-canonical polygon etc.
  
  2. Neighbour lists are special types of lists that contain information about the neighbours of each polygon. The neighbours can be defined either by adjacency or by distance.
  
  3. Distance matrices contain the distances between different points and can be either weighted or un-weighted. The weighted distances are usually objective to each point and its neighbours. Meaning that the closer or farther a neighbour is from the point of focus, the weight of their distance changes according to an applied kernel. Usually in the case of STx data, like the ones generated by the 10X Visium platform, the un-weighted distance between is two points is in pixels and we acquire it from the `spaceranger` output.
  
### The `sf` objects
Package `sf` represents simple features as native R objects. All functions and methods in `sf` that operate on spatial data are prefixed by *st_*, which refers to *spatial type*. Simple features are implemented as R native data, using simple data structures (S3 classes, lists, matrix, vector). Typical use involves reading, manipulating and writing of sets of features, with attributes and geometries.

As attributes are typically stored in `data.frame` objects (or the very similar `tbl_df`), we will also store feature geometries in a `data.frame` column. Since geometries are not single-valued, they are put in a list-column, a list of length equal to the number of records in the `data.frame`, with each list element holding the simple feature geometry of that feature.  The three classes used to represent simple features are:

* `sf`, the table (`data.frame`) with feature attributes and feature geometries, which contains
* `sfc`, the list-column with the geometries for each feature (record), which is composed of
* `sfg`, the feature geometry of an individual simple feature.

#### Simple feature geometry types

The following seven simple feature types are the most common:

| type | description                                        |
| ---- | -------------------------------------------------- |
| `POINT` | zero-dimensional geometry containing a single point |
| `LINESTRING` | sequence of points connected by straight, non-self intersecting line pieces; one-dimensional geometry |
| `POLYGON` | geometry with a positive area (two-dimensional); sequence of points form a closed, non-self intersecting ring; the first ring denotes the exterior ring, zero or more subsequent rings denote holes in this exterior ring |
| `MULTIPOINT` | set of points; a MULTIPOINT is simple if no two Points in the MULTIPOINT are equal |
| `MULTILINESTRING` | set of linestrings |
| `MULTIPOLYGON` | set of polygons |
| `GEOMETRYCOLLECTION` | set of geometries of any type except GEOMETRYCOLLECTION |

Each of the geometry types can also be a (typed) empty set, containing zero coordinates (for `POINT` the standard is not clear how to represent the empty geometry). Empty geometries can be thought of being the analogue to missing (`NA`) attributes, NULL values or empty lists.

#### sf: objects with simple features

As we usually do not work with geometries of single `simple features`, but with datasets consisting of sets of features with attributes, the two are put together in `sf` (simple feature) objects.  The following command reads a test dataset called `nc` from a file that is contained in the `sf` package:

```{r 03_sf_LoadTest}
nc <- st_read(system.file("shape/nc.shp", package = "sf"))
```

The short report printed gives the file name, the driver (ESRI Shapefile), mentions that there are 100 features (records, represented as rows) and 14 fields (attributes, represented as columns). 

This object is of class:
```{r 03_sf_TestClass}
class(nc)
```

meaning it extends (and "is" a) `data.frame`, but with a single list-column with geometries, which is held in the column with name:

```{r 03_sf_Test_sf_column}
attr(nc, "sf_column")
```

If we print the first three features, we see their attribute values and an abridged version of the geometry

```{r 03_sf_Test_print, echo=TRUE, eval=FALSE}
print(nc[9:15], n = 3)
```

which would give the following output:

```{r Sf-overview, echo=FALSE, out.width = "100%", fig.align="center", fig.cap="Overview of the `sf` object."}
knitr::include_graphics("images/sf_xfig.png")
```

In the output we see:

* in green a simple feature: a single record, or `data.frame` row, consisting of attributes and geometry
* in blue a single simple feature geometry (an object of class `sfg`)
* in red a simple feature list-column (an object of class `sfc`, which is a column in the `data.frame`)
* that although geometries are native R objects, they are printed as [well-known text](#wkb)

It is also possible to create `data.frame` objects with geometry list-columns that are not of class `sf`, e.g. by:
```{r 03_sf_no.sf}
nc.no_sf <- as.data.frame(nc)
class(nc.no_sf)
```

However, such objects:

* no longer register which column is the geometry list-column
* no longer have a plot method, and
* lack all of the other dedicated methods for class `sf`

#### sfc: simple feature geometry list-column

The column in the `sf` data.frame that contains the geometries is a list, of class `sfc`. We can retrieve the geometry list-column in this case by using standard `data.frame` notation like `nc$geom` or `nc[[15]]`, but the more general way uses `st_geometry`:

```{r 03_sfc_Test1}
(nc_geom <- st_geometry(nc))
```

Geometries are printed in abbreviated form, but we can view a complete geometry by selecting it, e.g. the first one by:

```{r 03_sfc_Test2}
nc_geom[[1]]
```

The way this is printed is called *well-known text*, and is part of the standards. The word `MULTIPOLYGON` is followed by three parentheses, because it can consist of multiple polygons, in the form of `MULTIPOLYGON(POL1,POL2)`, where `POL1` might consist of an exterior ring and zero or more interior rings, as of `(EXT1,HOLE1,HOLE2)`. Sets of coordinates are held together with parentheses, so we get `((crds_ext)(crds_hole1)(crds_hole2))` where `crds_` is a comma-separated set of coordinates of a ring. This leads to the case above, where `MULTIPOLYGON(((crds_ext)))` refers to the exterior ring (1), without holes (2), of the first polygon (3) - hence three parentheses.

We can see there is a single polygon with no rings:

```{r 03_sfc_Test3, fig.height=3}
par(mar = c(0,0,1,0))
plot(nc[1], reset = FALSE) # reset = FALSE: we want to add to a plot with a legend
plot(nc[1,1], col = 'grey', add = TRUE)
```

Following the `MULTIPOLYGON` data structure, in R we have a list of lists of lists of matrices. For instance,
we get the first 3 coordinate pairs of the second exterior ring (first ring is always exterior) for the geometry
of feature 4 by:

```{r 03_sfc_Test4}
nc_geom[[4]][[2]][[1]][1:3,]
```

Geometry columns have their own class,

```{r 03_sfc_Test5}
class(nc_geom)
```

#### sfg: simple feature geometry

Simple feature geometry (`sfg`) objects carry the geometry for a single feature, e.g. a point, linestring or polygon.

Simple feature geometries are implemented as R native data, using the following rules

1. a single POINT is a numeric vector
2. a set of points, e.g. in a LINESTRING or ring of a POLYGON is a `matrix`, each row containing a point
3. any other set is a `list`

```{r 03_sf_Test6, echo=FALSE, eval=TRUE, message=FALSE}
p <- rbind(c(3.2,4), c(3,4.6), c(3.8,4.4), c(3.5,3.8), c(3.4,3.6), c(3.9,4.5))
(mp <- st_multipoint(p))
s1 <- rbind(c(0,3),c(0,4),c(1,5),c(2,5))
(ls <- st_linestring(s1))
s2 <- rbind(c(0.2,3), c(0.2,4), c(1,4.8), c(2,4.8))
s3 <- rbind(c(0,4.4), c(0.6,5))
(mls <- st_multilinestring(list(s1,s2,s3)))
p1 <- rbind(c(0,0), c(1,0), c(3,2), c(2,4), c(1,4), c(0,0))
p2 <- rbind(c(1,1), c(1,2), c(2,2), c(1,1))
pol <-st_polygon(list(p1,p2))
p3 <- rbind(c(3,0), c(4,0), c(4,1), c(3,1), c(3,0))
p4 <- rbind(c(3.3,0.3), c(3.8,0.3), c(3.8,0.8), c(3.3,0.8), c(3.3,0.3))[5:1,]
p5 <- rbind(c(3,3), c(4,2), c(4,3), c(3,3))
(mpol <- st_multipolygon(list(list(p1,p2), list(p3,p4), list(p5))))
(gc <- st_geometrycollection(list(mp, mpol, ls)))
```

The below figure illustrates the different types of geometries:

```{r 03_sf_Test7, echo=FALSE, eval=TRUE}
par(mar = c(0.1, 0.1, 1.3, 0.1), mfrow = c(2, 3))
plot(mp, col = 'red')
box()
title("MULTIPOINT")
plot(ls, col = 'red')
box()
title("LINESTRING")
plot(mls, col = 'red')
box()
title("MULTILINESTRING")
plot(pol, border = 'red', col = 'grey', xlim = c(0,4))
box()
title("POLYGON")
plot(mpol, border = 'red', col = 'grey')
box()
title("MULTIPOLYGON")
plot(gc, border = 'grey', col = 'grey')
box()
title("GEOMETRYCOLLECTION")
par(mfrow = c(1, 1))
```

Geometries can also be empty, as in

```{r 03_sf_Test8, collapse=TRUE}
(x <- st_geometrycollection())
length(x)
```

*The above are taken from the very well written, well-descriptive and thorough `sf` package [vignette](https://cran.r-project.org/web/packages/sf/vignettes/sf1.html).*

## Data structures preparation
For this practical we will be using a human steatotic kidney dataset from the [Liver Atlas](https://livercellatlas.org/index.php) [@GUILLIAMS2022379]. 

### Load new dataset
First we load the counts table with genes in rows and spots in columns.
```{r 03_load_STx_counts}
## Load counts table
inputD <- readRDS(file = "./data/hsLivSteat_JBO019_inputD.rds")
inputD[1:5, 1:3]
```

Then we load the spot metadata. The imported table has the below columns:

* **Barcode:** the spot barcode (*note that we substituted the "-" with a "." to match the column names in the inputD because it is not good practise to have "-" in column names in R*).  
* **Section:** whether the spot is on- or off- tissue (on-tissue = 1, off-tissue = 0).  
* **Spot_Y/ Spot_X:** X and Y spot location on the capture area array.  
* **Image_Y/ Image_X:** X and Y spot coordinates in the full resolution image.  
* **pixel_x/ pixel_y:** X and Y spot coordinates in the low resolution image.  
```{r 03_load_STx_spotMetaData}
## Load spot metadata
inputMD <- readRDS(file = "./data/hsLivSteat_JBO019_inputMD.rds")
head(inputMD)
```

### Create point geometries
First we want to extract the pixel coordinates from the `inputMD` data frame and then we will generate the point geometries (centroids) from all spots of the 10X Visium capture area. This helps to tessellate space better in the next step.

```{r 03_pointGeom, collapse=TRUE}
## Extract coordinates
spot_position <- inputMD %>% 
    select(c("Barcode", "pixel_x", "pixel_y", "Section"))

head(spot_position, 5)

## Convert spots to centroids
centroids <- spot_position %>% 
  st_as_sf(coords = c("pixel_x", "pixel_y"), 
           remove = FALSE)
head(centroids, 5)
```

### Tesselate space
Here we will take the steps towards tessellating space. This tessellation, brakes the area that surrounds the spots and as a result we can use it to find neighbours by adjacency later on. We need to always keep in mind that although tessellation makes the spots have common borders, the 10X Visium spots have a distance between them that is approximately 50m.

- First, we combine the points we calculated earlier into a multipoint geometry.
- Second, we tessellate space around the points using the Voronoi tessellation.
- Third, we can cut the tessellation around the edges because tessellation extends to infinity.

```{r voronoi-tessellation}
## Combine the points into a multipoint geometry:
cntd_union <- st_union(centroids)
head(cntd_union)

## Use the union of points to generate a voronoi object
voronoi <- st_voronoi(cntd_union, bOnlyEdges = TRUE)
head(voronoi)

## Create an enveloped voronoi tessellation around the tissue
voronoi_env <- st_intersection(st_cast(voronoi), st_convex_hull(cntd_union))
head(voronoi_env)
```

```{r plot_voronoi, fig.show = 'hold', out.width="50%", fig.height=5, fig.width=4}
## Plot tessellation as is
ggplot(data = voronoi) + geom_sf() + labs(title = "Tessellation") + theme_void()
## Plot enveloped tessellation
ggplot(data = voronoi_env) + 
  geom_sf() + 
  labs(title = "Tessellation cut to capture area") + 
  theme_void()
```

### Polygonise the tessellation

Here we will extract polygons from the tessellation object only for the spots that are on-tissue. Meaning, they have a value of **1** in the `Section` column.
```{r polygonise}
## Generate the POLYGONS from the MULTILINESTRING
polygons <- st_polygonize(voronoi_env) %>% # polygonise the tessellation
    st_cast() %>% # convert GEOMETRYCOLLECTION to multiple POLYGONS
    st_sf() %>%  # convert sfc object to sf for st_join afterwards
    st_join(., 
            centroids[centroids$Section == 1,],
            join = st_contains,
            left = FALSE) %>%
    rename(geom_pol = geometry) %>% # Join the centroids with the POLYGONS 
    mutate(Barcode_rn = Barcode) %>% # duplicate the barcode column
    column_to_rownames("Barcode_rn") %>% # move duplicate column to row names
    st_sf() # convert back to sf (mutate makes it a df)

head(polygons)
```
As you may have observed we joined the `polygons` object with the `centroids` object so that we can add the rest of the information like `Barcode`, `pixel_x`, and  `pixel_y`.

If we plot it:
```{r plot_map1, fig.height=5, fig.width=4}
## Plot on tissue polygons
ggplot(data = polygons) + geom_sf(aes(geometry = geom_pol)) + theme_void()
```

Because there are times that we will need the polygons and times that we will need their centroids we can add both geometries in the `polygons` object.
```{r 03_geoms-PolCnt, message=FALSE}
## Update the polygon object to keep the centroid geometries as well
polygons <- polygons %>% # rename polygons geom column
  left_join(as.data.frame(centroids)) %>% # left joint pols and cntds
  rename(geom_cntd = geometry) %>% # rename centroids geom column
  st_sf(sf_column_name = "geom_pol") # set polygons geom column to be the default (one must be)

head(polygons)
```

As we can see here we have two geometry columns; one named `geom_pol` and one named `geom_cntd`. As a result we need to have one of these two as the active geometry column for the `polygons` object of class `sf`. We can see at the top of the printed output that the active geometry column is `geom_pol`. In general we can switch between geometry columns using: `st_geometry(sf_object) <- "geom_column_to_change_to"`

### Identify neighbours
#### By contiguity
We can contiguity-based neighbours for each spot using the `poly2nb` function from `spdep`. The function is using heuristics to identify polygons sharing boundary points as neighbours. It also has a `snap =` argument, to allow the shared boundary points to be a short distance from one another. Here we select `snap = 0` because the tessellation generated polygons with shared borders. Finally, the `queen =` argument is set to `TRUE`. This means that the function will look for a chess queen-style of contiguities.
```{r get_adjacent_neighbours_a, message=FALSE, collapse=TRUE}
## Get contiguity neighbours
nb_adjc <- poly2nb(pl = polygons,
                   snap = 0,
                   queen = TRUE)
length(nb_adjc)
head(nb_adjc)
```
```{r get_adjacent_neighbours_b, message=FALSE, fig.show = 'hold', out.width="50%", fig.height=4, fig.width=5}
nb_adjc_n <- card(nb_adjc)

ggplot() + 
  geom_histogram(aes(x = nb_adjc_n, y = after_stat(density)), 
                 colour = "black", 
                 fill = "grey") +
  geom_density(aes(x = nb_adjc_n),
               alpha = 0.5,
               adjust = 0.5,
               fill = "#A0CBE8",
               colour = "#4E79A7") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 6)) + 
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  xlab("Number of Neighbours") +
  theme_classic()

ggplot() + 
  geom_sf(data = polygons, 
          aes(geometry = geom_pol), 
          colour = "grey30", fill = "white") +
  geom_sf(data = as(nb2lines(nb_adjc, coords = polygons$geom_cntd), "sf"), 
          colour = "black", linewidth = 0.25) +
  geom_sf(data = polygons,
          aes(geometry = geom_cntd),
          colour = "black", size = 1) +
  theme_void()
```

As you can see, the neighbours object is a `list` of length equal to the number of spots we have in the dataset. Each element of the list represents a spot and the numbers stored inside the indexes of the neighbouring spots and the spots have a different numbers of neighbours. The connectivity histogram visualises the distribution of the number of neighbours in the data.

#### By graph
Once representative points are available, the criteria for neighbourhood can be extended from just contiguity to include graph measures, distance thresholds, and -nearest neighbours.

The most direct graph representation of neighbours is to make a Delaunay triangulation of the points, which extends outwards to the convex hull of the points. Note that graph-based representations construct the interpoint relationships based on Euclidean distance. Because it joins distant points around the convex hull, it may be worthwhile to thin the triangulation as a Sphere of Influence (SOI) graph, removing links that are relatively long. Points are SOI neighbours if circles centred on the points, of radius equal to the points nearest neighbour distances, intersect in two places [@Avis1985].

```{r get_graph_neighbours1a, message=FALSE, collapse=TRUE}
## Set centroids as default geometry
st_geometry(polygons) <- "geom_cntd"
## Get the neighbour names
nb_names <- polygons$Barcode

## By Delaunay triangulation
nb_tri <- tri2nb(polygons$geom_cntd, row.names = nb_names)
length(nb_tri)
head(nb_tri)
```
```{r get_graph_neighbours1b, message=FALSE, fig.show = 'hold', out.width="50%", fig.height=4, fig.width=5}
nb_tri_n <- card(nb_tri)

ggplot() + 
  geom_histogram(aes(x = nb_tri_n, y = after_stat(density)), 
                 colour = "black", 
                 fill = "grey") +
  geom_density(aes(x = nb_tri_n),
               alpha = 0.5,
               adjust = 0.5,
               fill = "#A0CBE8",
               colour = "#4E79A7") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) + 
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  xlab("Number of Neighbours") +
  theme_classic()

ggplot() + 
  geom_sf(data = polygons, 
          aes(geometry = geom_pol), 
          colour = "grey30", fill = "white") +
  geom_sf(data = as(nb2lines(nb_tri, coords = polygons$geom_cntd), "sf"), 
          colour = "black", linewidth = 0.25) +
  geom_sf(data = polygons,
          aes(geometry = geom_cntd),
          colour = "black", size = 1) +
  theme_void()
```
```{r get_graph_neighbours2a, message=FALSE, collapse=TRUE}
## Get neighbours by SOI
nb_soi <- graph2nb(soi.graph(nb_tri, polygons$geom_cntd), 
                   row.names = nb_names)
length(nb_soi)
head(nb_soi)
nb_soi_n <- card(nb_soi)
```
``````{r get_graph_neighbours2b, message=FALSE, fig.show = 'hold', out.width="50%", fig.height=4, fig.width=5}
ggplot() + 
  geom_histogram(aes(x = nb_soi_n, y = after_stat(density)), 
                 colour = "black", 
                 fill = "grey") +
  geom_density(aes(x = nb_soi_n),
               alpha = 0.5,
               adjust = 0.5,
               fill = "#A0CBE8",
               colour = "#4E79A7") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 8)) + 
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  xlab("Number of Neighbours") +
  theme_classic()

ggplot() + 
  geom_sf(data = polygons, 
          aes(geometry = geom_pol), 
          colour = "grey30", fill = "white") +
  geom_sf(data = as(nb2lines(nb_soi, coords = polygons$geom_cntd), "sf"), 
          colour = "black", linewidth = 0.25) +
  geom_sf(data = polygons,
          aes(geometry = geom_cntd),
          colour = "black", size = 1) +
  theme_void()
```
```{r get_graph_neighbours3a, message=FALSE, collapse=TRUE}
## Get neighbours by Gabriel graph
nb_gbn <- graph2nb(gabrielneigh(polygons$geom_cntd), 
                   row.names = nb_names)
length(nb_gbn)
head(nb_gbn)
nb_gbn_n <- card(nb_gbn)
```
```{r get_graph_neighbours3b, message=FALSE, fig.show = 'hold', out.width="50%", fig.height=4, fig.width=5}
ggplot() + 
  geom_histogram(aes(x = nb_gbn_n, y = after_stat(density)), 
                 colour = "black", 
                 fill = "grey") +
  geom_density(aes(x = nb_gbn_n),
               alpha = 0.5,
               adjust = 0.5,
               fill = "#A0CBE8",
               colour = "#4E79A7") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 4)) + 
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  xlab("Number of Neighbours") +
  theme_classic()

ggplot() + 
  geom_sf(data = polygons, 
          aes(geometry = geom_pol), 
          colour = "grey30", fill = "white") +
  geom_sf(data = as(nb2lines(nb_gbn, coords = polygons$geom_cntd), "sf"), 
          colour = "black", linewidth = 0.25) +
  geom_sf(data = polygons,
          aes(geometry = geom_cntd),
          colour = "black", size = 1) +
  theme_void()
```
```{r get_graph_neighbours4a, message=FALSE, collapse=TRUE}
## Get Relative graph neighbours
nb_rn <- graph2nb(relativeneigh(polygons$geom_cntd), 
                   row.names = nb_names)
length(nb_rn)
head(nb_rn)
nb_rn_n <- card(nb_rn)
```
```{r get_graph_neighbours4b, message=FALSE, fig.show = 'hold', out.width="50%", fig.height=4, fig.width=5}
ggplot() + 
  geom_histogram(aes(x = nb_rn_n, y = after_stat(density)), 
                 colour = "black", 
                 fill = "grey") +
  geom_density(aes(x = nb_rn_n),
               alpha = 0.5,
               adjust = 0.5,
               fill = "#A0CBE8",
               colour = "#4E79A7") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) + 
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  xlab("Number of Neighbours") +
  theme_classic()

ggplot() + 
  geom_sf(data = polygons, 
          aes(geometry = geom_pol), 
          colour = "grey30", fill = "white") +
  geom_sf(data = as(nb2lines(nb_rn, coords = polygons$geom_cntd), "sf"), 
          colour = "black", linewidth = 0.25) +
  geom_sf(data = polygons,
          aes(geometry = geom_cntd),
          colour = "black", size = 1) +
  theme_void()
```

Delaunay triangulation neighbours and SOI neighbours are symmetric by design  if  is a neighbour of , then  is a neighbour of . The Gabriel graph is also a subgraph of the Delaunay triangulation, retaining a different set of neighbours [@Matula1980]. It does not, however, guarantee symmetry; the same applies to Relative graph neighbours [@Toussaint1980Jan].

#### By distance
An alternative method is to choose the  nearest points as neighbours  this adapts across the study area, taking account of differences in the densities of areal entities. Naturally, in the overwhelming majority of cases, it leads to asymmetric neighbours, but will ensure that all areas have  neighbours.
```{r get_distance_neighbours1a, collapse=TRUE}
## Set centroids as default geometry
st_geometry(polygons) <- "geom_cntd"
## Get distance-based neighbours
nb_knn <- knn2nb(knearneigh(polygons, k = 6), 
                 row.names = nb_names)
length(nb_knn)
head(nb_knn)
```
```{r get_distance_neighbours1b, fig.height=4, fig.width=5}
ggplot() + 
  geom_sf(data = polygons, 
          aes(geometry = geom_pol), 
          colour = "grey30", fill = "white") +
  geom_sf(data = as(nb2lines(nb_knn, coords = polygons$geom_cntd), "sf"), 
          colour = "black", linewidth = 0.25) +
  geom_sf(data = polygons,
          aes(geometry = geom_cntd),
          colour = "black", size = 1) +
  theme_void()
```

As you can see, this neighbours object is also a `list` of length equal to the number of spots we have in the dataset with the same content as the one earlier. Also, in this list all spots have the same number of neighbours since we selected *k = 6* which means we have the 6 nearest by distance spots. Another interesting fact here is that spots that are not adjacent to each other now might be neighbours.

Another way to find neighbours by distance is to first generate a graph for *k = 1*. This way we can find the shortest and the longest distance needed for a spot to acquire one neighbour. Then, we can use another function called `dnearneigh` which is used to find neighbours with an interpoint distance, with arguments `d1` and `d2` setting the lower and upper distance bounds. There as a lower bound we can provide zero or the shortest distance a neighbour is found in our dataset and as an upper bound we can provide a value that is a function of the largest distance a neighbour is found in our dataset.
```{r get_distance_neighbours2a}
## Get distance-based neighbours for k = 1
nb_1nn <- knn2nb(knearneigh(polygons, k = 1), 
                 row.names = nb_names)
dsts <- unlist(nbdists(nb_1nn, polygons$geom_cntd))
summary(dsts)
max_1nn <- max(dsts)
min_1nn <- min(dsts)
```

As you can see, because the 10X Visium spots are equidistant, the majority of the distances is around 6 pixels. Additionally, the maximum distance is almost 20 times the minimum. This is due to the one spot we keep having on the left of our tissue area. As a result in our dataset we will use as upper bound a value that is a function of the minimum distance instead of the maximum.

Let's have a look at how the dataset looks with the maximum as upper boundary

```{r get_distance_neighbours2b_a}
nb_1nna <- dnearneigh(polygons$geom_cntd, d1 = 0, d2 = 1*max_1nn, row.names = nb_names)
str(nb_1nna[1:5])
nb_1nna_n <- card(nb_1nna)
```
```{r get_distance_neighbours2b_b, message=FALSE, fig.height=4, fig.width=5}
ggplot() + 
  geom_histogram(aes(x = nb_1nna_n, y = after_stat(density)), 
                 colour = "black", 
                 fill = "grey") +
  geom_density(aes(x = nb_1nna_n),
               alpha = 0.5,
               adjust = 0.5,
               fill = "#A0CBE8",
               colour = "#4E79A7") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) + 
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  xlab("Number of Neighbours") +
  theme_classic()
```

As you can see, the number of neighbours each spot acquires because we use the maximum distance as the upper bound is massive and as a result it will not plot correctly.

```{r get_distance_neighbours2c, fig.show = 'hold', out.width="33%", fig.height=5, fig.width=4}
nb_1nnb <- dnearneigh(polygons$geom_cntd, d1 = 0, d2 = 1*min_1nn, row.names = nb_names)
nb_1nnc <- dnearneigh(polygons$geom_cntd, d1 = 0, d2 = 1.5*min_1nn, row.names = nb_names)
nb_1nnd <- dnearneigh(polygons$geom_cntd, d1 = 0, d2 = 1.75*min_1nn, row.names = nb_names)

ps <- grep("nn[b,c,d]", names(.GlobalEnv), value = TRUE)

for (p in ps) {
  q <- get(p)
  (ggplot() + 
      geom_sf(data = polygons, 
          aes(geometry = geom_pol), 
          colour = "grey30", fill = "white") +
      geom_sf(data = as(nb2lines(q, coords = polygons$geom_cntd), "sf"), 
          colour = "black", linewidth = 0.1) +
      geom_sf(data = polygons,
          aes(geometry = geom_cntd),
          colour = "black", size = 0.25) +
      labs(title = p) +
      theme_void()) %>% print()
}
```
```{r get_distance_neighbours2d, fig.show = 'hold', out.width="33%", fig.height=5, fig.width=4}
for(p in ps) {
  q <- get(p)
  q_n <- card(q)

  (ggplot() + 
    geom_histogram(aes(x = q_n, y = after_stat(density)), 
                   colour = "black", 
                   fill = "grey") +
    geom_density(aes(x = q_n),
                 alpha = 0.5,
                 adjust = 0.5,
                 fill = "#A0CBE8",
                 colour = "#4E79A7") +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) + 
    scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
    labs(title = p, 
         x = "Number of Neighbours") +
    theme_classic()) %>% print()
}
```
Using the maximum as the upper limit is not prohibited, it is just not advised if you plan to plot the neighbour relationships on a map.

*The above for identifying neighbours are partially taken from the very well written, well-descriptive and thorough `spdep` package [vignette](https://r-spatial.github.io/spdep/articles/nb.html#distance-based-neighbours) for neighbours identification.*  

#### Adding spatial weights
The neighbour lists can be supplemented with spatial weights using the `nb2listw` and `nb2listwdist` function from `spdep` package for the chosen type and coding scheme style. There are 6 different coding scheme styles that can be used to weigh neighbour relationships:

1. **B**: is the basic binary coding (1 for neighbour, 0 for no neighbour).
2. **W**: is row standardised (sums over all links to n).
3. **C**: is globally standardised (sums over all links to n).
4. **U**: is equal to C divided by the number of neighbours (sums over all links to unity).
5. **S**: is the variance-stabilizing coding scheme (sums over all links to n).
6. **minmax**: divides the weights by the minimum of the maximum row sums and maximum column sums of the input weights; It is similar to the C and U styles.

The coding scheme style is practically the value each neighbour will get. For example, in a binary coding scheme style (**B**) if a spot is a neighbour of the spot in focus then gets the value of **1**, else gets **0**. Another example, in a row standardised coding scheme style (**W**) if the spot in focus has a total of 10 neighbours and each neighbour has a weight of 1, then the sum of all neighbour weights is 10, and each neighbour will get a normalised weight of 1/10 = 0.1. As a result, in the row standardised coding scheme, spots with many neighbours will have neighbours with lower weights and thus will not be over-emphasised.

Starting from a binary neighbours list, in which regions are either listed as neighbours or are absent (thus not in the set of neighbours for some definition), we can add a distance-based weights list. The `nb2listwdist` function supplements a neighbours list with spatial weights for the chosen types of distance modelling and coding scheme. While the offered coding schemes parallel those of the `nb2listw` function above, three distance-based types of weights are available: inverse distance weighting (IDW), double-power distance weights (DPD), and exponential distance decay (EXP). The three types of distance weight calculations are based on pairwise distances , all of which are controlled by parameter *alpha* ( below):

1. **idw**: =,
2. **exp**: =exp(),
3. **dpd**: =[1(/max)],

the latter of which leads to =0 for all >max. Note that *IDW* weights show extreme behaviour close to 0 and can take on the value infinity. In such cases, the infinite values are replaced by the largest finite weight present in the weights list.

The default coding scheme for `nb2listwdist` is raw, which outputs the raw distance-based weights without applying any kind of normalisation. In addition, the same coding scheme styles that are also available in the `nb2listw` function can be chosen.

Below we will use only the `nb2listwdist` function which can accommodate both weight types and coding scheme styles.

```{r 03_spatial_weights_to_nb}
## Set centroids as default geometry
st_geometry(polygons) <- "geom_cntd"
## Add weights
nb_adjc_w <- nb2listwdist(nb_adjc, polygons, type = "idw", style = "W", zero.policy = TRUE)
## Have a look
nb_adjc_w$weights[1:5]
```

### Generate distance matrices
A distance matrix is a mirrored matrix that contains the distance between a spot and every other spot. This distance can be a simple Euclidean distance based on the coordinates of the spots or a weighted distance according to a bandwidth around each spot using a kernel that gives higher scores to distances between spots that are closer together compared to the ones that are farther away. These weighted distance matrices are later used to run geographically weighted (GW) models.

There are 6 different kernels that can be used to weight the distances between spots. The next two figures are from the `GWmodel`'s publication [@Gollini2015Feb] and provide a bit more description on that.

```{r GWmodel_fig1, echo=FALSE, out.width = "100%", fig.align="center", fig.cap="The math equations that define the kernels."}
knitr::include_graphics("images/gwmodel_kernel_math.png")
```

```{r GWmodel_fig2, echo=FALSE, out.width = "100%", fig.align="center", fig.cap="Examples from using each kernel."}
knitr::include_graphics("images/gwmodel_kernel_graphs.png")
```

```{r 03 dist_matrix}
## Set centroids as default geometry
st_geometry(polygons) <- "geom_cntd"
## Get Euclidean distances between spots
dist.Mat <- gw.dist(dp.locat = st_coordinates(polygons), p = 2)
rownames(dist.Mat) <- nb_names
colnames(dist.Mat) <- nb_names
dist.Mat[1:5, 1:4]
## Set bandwidth
bw = (range(dist.Mat)[2])/2
bw
## Select a kernel
kernel = "bisquare"
## Calculate W distance matrix
w <- gw.weight(vdist = dist.Mat, 
               bw = bw,
               kernel = kernel,
               adaptive = FALSE)
rownames(w) <- nb_names
colnames(w) <- nb_names
w[1:5, 1:4]
```


### Putting it all together
The below code puts all these steps in order by selecting one of the options at each step.

```{r all_in_one}
## Load data and metadata
inputD <- readRDS(file = "./data/hsLivSteat_JBO019_inputD.rds")
inputMD <- readRDS(file = "./data/hsLivSteat_JBO019_inputMD.rds")
## Create point geometries
spot_position <- inputMD %>% 
    select(c("Barcode", "pixel_x", "pixel_y", "Section"))
centroids <- spot_position %>% 
  st_as_sf(coords = c("pixel_x", "pixel_y"), 
           remove = FALSE)
## Tessellate space
cntd_union <- st_union(centroids)
voronoi <- st_voronoi(cntd_union, bOnlyEdges = TRUE)
voronoi_env <- st_intersection(st_cast(voronoi), st_convex_hull(cntd_union))
## Polygonise tessellation
polygons <- st_polygonize(voronoi_env) %>% 
    st_cast() %>% 
    st_sf() %>%  
    st_join(., 
            centroids[centroids$Section == 1,],
            join = st_contains,
            left = FALSE) %>%
    rename(geom_pol = geometry) %>% 
    mutate(Barcode_rn = Barcode) %>% 
    column_to_rownames("Barcode_rn") %>% 
    st_sf() 
## Update the polygon object to keep the centroid geometries as well
polygons <- polygons %>% 
  left_join(as.data.frame(centroids)) %>% 
  rename(geom_cntd = geometry) %>% 
  st_sf(sf_column_name = "geom_pol") 
## Identify neighbours by Sphere Of Influence
st_geometry(polygons) <- "geom_cntd"
nb_tri <- tri2nb(polygons$geom_cntd, row.names = nb_names)
nb_soi <- graph2nb(soi.graph(nb_tri, polygons$geom_cntd), 
                   row.names = nb_names)
nb_soi_w <- nb2listwdist(nb_soi, polygons, type = "idw", style = "W", zero.policy = TRUE)
## Generate distance matrix
dist.Mat <- gw.dist(dp.locat = st_coordinates(polygons), p = 2)
rownames(dist.Mat) <- nb_names
colnames(dist.Mat) <- nb_names
bw = (range(dist.Mat)[2])/2
kernel = "bisquare"
dist.Mat.w <- gw.weight(vdist = dist.Mat, 
               bw = bw,
               kernel = kernel,
               adaptive = FALSE)
```

```{r all_in_one_plots, fig.show = 'hold', out.width="50%", fig.height=4, fig.width=5}
## Plot on tissue polygons
ggplot(data = polygons) + geom_sf(aes(geometry = geom_pol)) + theme_void()
## Plot neighbour graph
ggplot() + 
  geom_sf(data = polygons, 
          aes(geometry = geom_pol), 
          colour = "grey30", fill = "white") +
  geom_sf(data = as(nb2lines(nb_soi, coords = polygons$geom_cntd), "sf"), 
          colour = "black", linewidth = 0.25) +
  geom_sf(data = polygons,
          aes(geometry = geom_cntd),
          colour = "black", size = 1) +
  theme_void()
```

<!--chapter:end:03-intro-geospatial.Rmd-->

# Practical session 2
In this session we will have a hands-on exploration of GW-PCA and its application to STx data. What can we learn from this novel technique?

## Geographically Weighted Principal Components Analysis (GWPCA)
A standard PCA can pick out the key multivariate modes of variability in the data. Looking at outlying values of the principal components of these data gives us an idea of unusual sites (in terms of combinations of gene expression profiles -and to a certain extend of combinations of cell types in each spot). Next, Geographically weighted PCA can be used to find spatial multivariate outliers. Sounds complicated, but really all this means is it identifies sites that have an unusual multi-way combination of gene expression in relation to their immediate geographical neighbours. It might be that the values observed at these sites as a combination is not uncommon in the tissue as a whole - but is very unusual in its locality.

To find such outliers the procedure is relatively simple - instead of doing a PCA on the tissue as a whole, for each sample we do a PCA on data falling into a window centred on the location of that spot. In that way we can check whether the spot is like its neighbours or not, from a multivariate viewpoint.

The following code carries out a geographically weighted PCA. In short, it runs a windowed PCA around each of the spots.

## Load packages
```{r 04_loadPackages, , message=FALSE}
library(spdep)
library(sf)
library(GWmodel)
library(ggplot2)
library(tidyverse)
library(cols4all)
library(scuttle)
library(scater)
library(scran)
```

## Load Quality Controled and Normalised data
```{r QC_Norm_LiverData, echo=FALSE, message=FALSE, warning=FALSE, results='hide', eval=FALSE}
## Keep on-tissue spots
colDATA <- inputMD %>%
  dplyr::filter(Section == 1)

## Ground truth from dataset authors
groundTruth <- read.table("./data/spotzonationGroup.txt", header = TRUE)
colDATA <- colDATA %>%
  left_join(groundTruth)

## Get mitochondrial gene names
#biomartHumanGRCh37 <- create_biomart("human", version = "GRCh37")
biomartHumanGRCh37 <- readRDS(file = "./data/biomartHumanGRCh37.rds")
#rowDATA <- annotate_vector(rownames(inputD), biomartHumanGRCh37)
rowDATA <- readRDS(file = "./data/rowDATA.rds")
is_mito <- grepl("(^MT-)|(^mt-)", rowDATA$gene_name)

## Calculate per-spot QC metrics and store in colData
perCellQC <- perCellQCMetrics(inputD, subsets = list(mito = is_mito)) %>% 
  as.data.frame() %>%
  rownames_to_column(var = "Barcode")
colDATA <- colDATA %>% 
  left_join(perCellQC)

## Add geometries to colData
colDATA <- colDATA %>% 
  left_join(polygons)

## Select library size threshold
qc_lib_size <- colDATA$sum < 700
colDATA$qc_lib_size <- qc_lib_size
## Select expressed genes threshold
qc_detected <- colDATA$detected < 500
colDATA$qc_detected <- qc_detected
## Select expressed genes threshold
qc_mito <- colDATA$subsets_mito_percent > 24
colDATA$qc_mito <- qc_mito
## Check the number of discarded spots for each metric
apply(cbind(qc_lib_size, qc_detected, qc_mito), 2, sum)
## Combine together the set of discarded spots
discard <- qc_lib_size | qc_detected | qc_mito
## Store the set in the object
colDATA$discard <- discard
## Vector to remove spots from counts table too
discard_c <- colnames(inputD) %in% colDATA[!colDATA$discard,"Barcode"]

## Remove spots that fail to pass the QC
colDATA <- colDATA[!colDATA$discard,]
counts <- inputD[,discard_c]
identical(colDATA$Barcode, colnames(counts))

## Calculate size factors
sizeFactor <- librarySizeFactors(counts)

## Normalise counts
counts_norm <- normalizeCounts(counts, size.factors = sizeFactor)

## Remove mitochondrial genes
counts_norm <- counts_norm[!is_mito, ]
rowDATA <- rowDATA[!is_mito, ]

## Fit mean-variance relationship
dec <- modelGeneVar(counts_norm)

## Select top HVGs
top_hvgs <- getTopHVGs(dec, prop = 0.1)
rowDATA$topHVGs <- rowDATA$id %in% top_hvgs

## Scale normalised counts 
counts_topHVGs <- counts_norm[rowDATA$topHVGs,] %>% 
  t()
countsNormCenter <- scale(counts_topHVGs, center = TRUE, scale = TRUE)

## Add size factors in colData
sizeFactor <- sizeFactor %>% 
  as.data.frame() %>%
  rename("sizeFactor" = ".") %>%
  rownames_to_column(var = "Barcode")
colDATA <- colDATA %>% 
  left_join(sizeFactor)

## Save
saveRDS(countsNormCenter, file = "./data/countsNormScaled.rds")
saveRDS(rowDATA, file = "./data/rowDATA.rds")
saveRDS(colDATA, file = "./data/colDATA.rds")
```

```{r 04_load_data}
countsNormCenter <- readRDS(file = "./data/countsNormScaled.rds")
rowDATA <- readRDS(file = "./data/rowDATA.rds")
colDATA <- readRDS(file = "./data/colDATA.rds")
```

We imported 3 tables:

1. `countsNormCenter`: normalised and centred and scaled counts for the top HVGs without mitochondrial genes. We followed the preprocessing steps from practical session 2.
2. `rowDATA`: Gene annotations (containing, amongst others, ENSG IDs and gene names).
3. `colDATA`: Metadata for the spots that passed the QC as in practical session 2.

Because gwpca uses princomp to run the PCAs and this does not accept the number of variables (genes) being more than the number of samples (spots).

## Data prearation and single GWPCA run
```{r 04_prepare_gwpca}
## Prepare for Geographically Weighted PCA (GWPCA)
countsNormCenter <- countsNormCenter %>%
  as.data.frame() %>%
  rownames_to_column(var = "rowname") %>%
  arrange("rowname") %>%
  column_to_rownames("rowname")

## Get the coordinates
coords <- colDATA[, c("Barcode", "pixel_x", "pixel_y")] %>%
  arrange(Barcode) %>%
  column_to_rownames(var = "Barcode")

## Get the data into a SpatialPointsDataFrame object
inputPCAgw <- SpatialPointsDataFrame(coords, 
                                     countsNormCenter, 
                                     match.ID = TRUE)
```

```{r 04_set_parameters}
## Get the gene names that are going to be evaluated
vars <- colnames(inputPCAgw@data)

## Set the number of components to be retained
k <- 20

## Set the kernel to be used
kernel = "gaussian"

## Set a bandwidth -in pixels- for neighbourhood 
dist.Mat <- gw.dist(dp.locat = st_coordinates(colDATA$geom_cntd), p = 2)
```

The bandwidth is essentially the radius around each spot where every other spot that falls inside it is considered a neighbour. We can set bandwidth as a fixed value or we can select the bandwidth automatically. Without going into detail here, this is achieved by a form of cross validation, where each observation is omitted, and it is attempted to reconstruct the values on the basis of principal components, derived from the other observations. The bandwidth achieving the optimal results is the one selected. For a complete explanation, see @Harris2011Oct. The function `bw.gwpca` computes this:
```{r 04_bw_choice, eval=FALSE}
bw.choice <- bw.gwpca(inputPCAgw, 
                      vars = vars, 
                      k = k,
                      kernel = kernel,
                      dMat = dist.Mat, 
                      adaptive = TRUE)

## Save 
saveRDS(bw.choice, file = "./data/bw.choice.rds")
```

Since this command computes several GWPCA estimates at different bandwidths in order to find an optimum, it will take noticeably longer than the version of the function with a prescribed bandwidth. Once run, `gwpca` is re-run with the bandwidth set to `bw.choice.` Below load the `bw.choice` object we already calculated for you:
```{r 04_bw_choice_load}
bw.choice <- readRDS(file = "./data/bw.choice.rds")
```

- **NOTE**: Larger bandwidths imply bigger moving spatial windows, which in turn imply smoother spatially varying outputs.

## Run GWPCA
Run the optimised GWPCA with the automatically estimated bandwidth:
```{r 04_run_gwpca1, eval=FALSE}
pcaGW <- gwpca(inputPCAgw,
               vars = vars,
               bw = bw.choice,
               k = k,
               dMat = dist.Mat,
               adaptive = TRUE,
               kernel = kernel)
```

Because GWPCA can take some time to run, we ran it for you and below you can load the output:
```{r 04_run_gwpca2}
pcaGW <- readRDS(file = 
                   )
```

## Plot global PCA results
In the next steps we will be looking inside the output from `gwpca` function and we are going to extract some basic information. Since GWPCA is multiple local PCAs, it is good to know how many PCs makes sense to look at. We can do so by running a global PCA and plotting a scree plot:
```{r 04_scree_plot}
## Prepare data for scree plot
pvar <- pcaGW$pca$sdev^2/sum(pcaGW$pca$sdev^2)
pvar <- data.frame(var = pvar,
                   PCs = sprintf("PC%02d", seq(1, length(pvar))))

## Plot scree plot
ggplot(pvar[1:10,], aes(x = PCs, y = var, group = 1)) + 
    geom_point(size = 3) +
    geom_line() +
    xlab("Principal Component") +
    ylab("% Variance Explained") +
    ggtitle("Scree Plot") +
    ylim(0, 1) + 
    theme_classic()
```
In a Principal Component Analysis (PCA), the first three principal components may explain less than 15% of the variance in the data if the data is highly dispersed or if there is a large amount of noise in the data. This means that the first three principal components are not capturing a significant portion of the variability in the data. This could be due to a lack of clear structure in the data or a lack of meaningful patterns that can be captured by the PCA. Alternatively, it could be due to the presence of many irrelevant features or variables in the data that are not contributing to the overall variance.

## Identify the leading genes in each location

```{r leading_genes}
## Load biomart
biomartHumanGRCh37 <- readRDS(file = "./data/biomartHumanGRCh37.rds")

## Extract leading genes
lead.item <- gwpca.Leading.G.single(pcaGW, 
                                    pc.no = 1, 
                                    sf.geom = colDATA$geom_pol,
                                    gene.names = TRUE,
                                    biomart = biomartHumanGRCh37,
                                    check.names = FALSE)

pc.No = 1
col.No <- length(unique(lead.item[,1]))
colour.values <- get.colours(col.No)
    
ggplot() + 
  geom_sf(data = lead.item$geometry, 
          aes(fill = lead.item[,1])) +
  scale_fill_manual(values = colour.values) + 
  xlab("X coordinates (pixels)") +
  ylab("Y coordinates (pixels)") +
  labs(title = paste0("Leading Genes on PC", pc.No),
       fill = "Leading Genes") +
  theme_void() + 
  theme(legend.position = "none")
```

## Percentage of Total Variation (PTV)
Another useful diagnostic for PCA is the percentage of variability in the data explained by each of the components. This can be achieved by looking at the `var` component of `pcaGW`; this is written as `pcaGW$var`. This is an XXXX by XX matrix - where XXXX is the number of observations and XX is the number of components. For each location, the XX columns correspond to the variance of each of the principal components. Looking at the proportion of each component in the sum of all of the variances shows how much of the variability in the data each component contributes. If, say, the first two components contributed 90% of the total variance, then it is reasonable to assume that much of the variability in the data can be seen by just looking at these two components. Because this is geographically weighted PCA, however, this quantity varies across the map.
```{r 04_ptv}
## Calculate the PTV for multiple Components
props <- gwpca.prop.var(gwpca.obj = pcaGW,
                        n.comp = c(5, 10, 20, 30, 40, 50),
                        polygons = colDATA$geom_pol)

## Map PTV
for (i in c(5, 10, 20, 30)) {
    comps <- sprintf("Comps_%02d", i)
    ptv.map <- dplyr::select(props, all_of(c(comps, "geometry")))
    
    (ggplot() + 
      geom_sf(data = ptv.map$geometry,
              aes(fill = ptv.map[,1])) +
      scale_fill_viridis_c(option = "inferno", limits = c(0, 100)) +
      labs(title = "Percantage of Total Variation\n(PTV)",
           fill = paste0("PTV of ", i, "\n components")) +
      theme_void() +
      theme(legend.position = "right")) %>% 
      print()
}
```

## Identify discrepancies
Global PCA can be used to identify multivariate outliers. Extending this, it is also possible to use local PCA (i.e., GWPCA) to identify local outliers. One way of doing this links back to the cross-validation idea used earlier to select a bandwidth. Recall that this is based on a score of how well each observation can be reconstructed on the basis of local PCs. The score measures the total discrepancies of true data values from the reconstructed ones - and the bandwidth chosen is the one minimising this. However, the total discrepancy score is the sum of the individual discrepancies. A very large individual discrepancy associated with an observation suggests it is very different - in a multidimensional way, to the observations near to it. These discrepancies can be calculated with the `gwpca.cv.contrib` function.
```{r 04_discrep1}
## Calculate the discrepancies
data.mat <- as.matrix(inputPCAgw@data)
discrepancy <- gwpca.cv.contrib(data.mat, 
                                coordinates(inputPCAgw),
                                bw = bw.choice,
                                adaptive = TRUE, 
                                dMat = dist.Mat)
discrepancy_df <- data.frame(disc = discrepancy)
```

```{r 04_discrep2}
## Plot as boxplot
ggplot(pivot_longer(discrepancy_df, col = "disc"),
       aes(x = name, y = value)) + 
    geom_boxplot(fill = "#D1E5F0", colour = "#2166AC", 
                 outlier.colour = "red", outlier.size = 2) + 
    geom_jitter(col = "#EF8A62", size = 2, width = 0.3, alpha = 0.8) +
    # add horizontal line
    geom_hline(yintercept = c(2e+04, 1.8e+05), linetype = "dashed", color = "royalblue") +
    # customise axes
    scale_x_discrete(labels = "Locations") +
    coord_flip() +
    xlab(NULL) +
    ylab("Local PC Discrepancy") +
    theme_classic() + 
    theme(axis.text.y = element_text(angle = 90, hjust = 0.5))
```

Comment on the discrepancies and the Liver histopathology

```{r 04_discrep3}
## Plot map
dt <- inputPCAgw@data %>%
    mutate(disc = discrepancy,
           geometry = colDATA$geom_pol)

disc.map <- dplyr::select(dt, all_of(c("disc", "geometry")))

ggplot() + 
  geom_sf(data = disc.map$geometry, 
          aes(fill = disc.map$disc)) + 
  scale_fill_viridis_c(option = "inferno") +
  labs(title = "Local PC Discrepancy",
         fill = "Discrepancy\nscore") +
  theme_void() + 
  theme(legend.position = "right")
```

Another possibility to understand the nature of the outlier is a parallel coordinates heatmap. Here, each observation neighbouring the location that has been found to be an outlier is shown as a column with the genes in rows. Since here we are investigating local outliers, one particular observation is highlighted in red -the outlier-, and the remaining ones in grey, but with the intensity of the grey fading according to their distance from the red observation. This enables you to see what characteristic the red observation has that means it as outlying from its neighbours. The plot can be created using `gw.pcplot`:

```{r 04_discrep4}
# Get the highest-scoring outliers
outliers <- which(discrepancy_df > 1.8e+05)

# Plot the heatmap to visualise the genes that make this location an outlier
gwpca.plot.outlier(countsNormCenter,
                   bw = bw.choice,
                   focus = outliers[1],
                   dMat = dist.Mat,
                   show.vars = "top",
                   mean.diff = 1,
                   gene.names = TRUE,
                   biomart = biomartHumanGRCh37,
                   show.data = FALSE,
                   check.names = FALSE,
                   scale = "row",
                   color = rev(colorRampPalette(brewer.pal(11, "RdBu"))(1000)))
```

<!--chapter:end:04-gwpca.Rmd-->

# Blocks

## Equations

Here is an equation.

\begin{equation} 
  f\left(k\right) = \binom{n}{k} p^k\left(1-p\right)^{n-k}
  (\#eq:binom)
\end{equation} 

You may refer to using `\@ref(eq:binom)`, like see Equation \@ref(eq:binom).


## Theorems and proofs

Labeled theorems can be referenced in text using `\@ref(thm:tri)`, for example, check out this smart theorem \@ref(thm:tri).

::: {.theorem #tri}
For a right triangle, if $c$ denotes the *length* of the hypotenuse
and $a$ and $b$ denote the lengths of the **other** two sides, we have
$$a^2 + b^2 = c^2$$
:::

Read more here <https://bookdown.org/yihui/bookdown/markdown-extensions-by-bookdown.html>.

## Callout blocks


The R Markdown Cookbook provides more help on how to use custom blocks to design your own callouts: https://bookdown.org/yihui/rmarkdown-cookbook/custom-blocks.html

<!--chapter:end:05-blocks.Rmd-->

# Sharing your book

## Publishing

HTML books can be published online, see: https://bookdown.org/yihui/bookdown/publishing.html

## 404 pages

By default, users will be directed to a 404 page if they try to access a webpage that cannot be found. If you'd like to customize your 404 page instead of using the default, you may add either a `_404.Rmd` or `_404.md` file to your project root and use code and/or Markdown syntax.

## Metadata for sharing

Bookdown HTML books will provide HTML metadata for social sharing on platforms like Twitter, Facebook, and LinkedIn, using information you provide in the `index.Rmd` YAML. To setup, set the `url` for your book and the path to your `cover-image` file. Your book's `title` and `description` are also used.



This `gitbook` uses the same social sharing data across all chapters in your book- all links shared will look the same.

Specify your book's source repository on GitHub using the `edit` key under the configuration options in the `_output.yml` file, which allows users to suggest an edit by linking to a chapter's source file. 

Read more about the features of this output format here:

https://pkgs.rstudio.com/bookdown/reference/gitbook.html

Or use:

```{r eval=FALSE}
?bookdown::gitbook
```



<!--chapter:end:06-share.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:07-references.Rmd-->

